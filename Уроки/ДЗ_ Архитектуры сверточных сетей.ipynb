{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7J2ylU2s8v5"
      },
      "source": [
        "## Архитектуры свёрточных сетей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T16:19:19.017474Z",
          "start_time": "2019-11-18T16:19:18.970512Z"
        },
        "id": "y0aUeYwa07Np"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision as tv\n",
        "from torchsummary import summary\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T16:20:21.306023Z",
          "start_time": "2019-11-18T16:20:21.293827Z"
        },
        "id": "uwaCmLfh07Nx"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0, 0\n",
        "    net.eval()\n",
        "    for X, y in data_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
        "        n += y.shape[0]\n",
        "    return acc_sum.item() / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T16:20:21.693926Z",
          "start_time": "2019-11-18T16:20:21.678441Z"
        },
        "id": "mHmVyS9O07Nz"
      },
      "outputs": [],
      "source": [
        "def train(net, train_iter, test_iter, trainer, num_epochs):\n",
        "    net.to(device)\n",
        "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
        "    net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        \n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            trainer.zero_grad()\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.shape[0]\n",
        "\n",
        "            if i % 10 == 0:\n",
        "              print(f\"Step {i}. time since epoch: {time.time() -  start:.3f}. \" \n",
        "                    f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
        "        test_acc = evaluate_accuracy(test_iter, net.to(device))\n",
        "        print('-' * 20)\n",
        "        print(f'epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}'\n",
        "              f', test acc {test_acc:.3f}, time {time.time() - start:.1f} sec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xjRzZazUBBhE",
        "outputId": "94337aed-388f-49cd-cdcf-c491c89d31de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#torch.backends.cuda.max_split_size_mb = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaVsexlm07Ns"
      },
      "source": [
        "## DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T16:24:00.569981Z",
          "start_time": "2019-11-18T16:24:00.510393Z"
        },
        "id": "E0-WXQPV07N6"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 192\n",
        "transoforms = tv.transforms.Compose([\n",
        "    tv.transforms.Resize((224, 224)),\n",
        "    tv.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = tv.datasets.EMNIST('../data', split='balanced', train=True, download=True, transform=transoforms)\n",
        "test_dataset = tv.datasets.EMNIST('../data', split='balanced', train=False, download=True, transform=transoforms)\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g55sb1uP07ON"
      },
      "source": [
        "## Предобученные архитектуры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:21:46.290923Z",
          "start_time": "2019-11-18T17:21:46.236514Z"
        },
        "id": "4l5GE7DA07OO"
      },
      "outputs": [],
      "source": [
        "transoforms = tv.transforms.Compose([\n",
        "    tv.transforms.Grayscale(3),\n",
        "    tv.transforms.Resize((224, 224)),\n",
        "    tv.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = tv.datasets.MNIST('.', train=True, transform=transoforms, download=True)\n",
        "test_dataset = tv.datasets.MNIST('.', train=False, transform=transoforms, download=True)\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MENekEq_AHqo"
      },
      "source": [
        "#### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:22:33.772924Z",
          "start_time": "2019-11-18T17:22:33.293818Z"
        },
        "id": "ju3bFRxI07OO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model = tv.models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STGg9M4sDuff",
        "outputId": "08347b00-c844-4fa9-9e5c-9f63d23607c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncajlZiCDwUt",
        "outputId": "896dcc46-ce0c-4739-bb4d-0c7bc4b78939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                 [-1, 1000]         513,000\n",
            "================================================================\n",
            "Total params: 11,689,512\n",
            "Trainable params: 11,689,512\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 44.59\n",
            "Estimated Total Size (MB): 107.96\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(model.to(device), input_size=(3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:23:47.319979Z",
          "start_time": "2019-11-18T17:23:47.316747Z"
        },
        "id": "QuYMuz8i07OO"
      },
      "outputs": [],
      "source": [
        "# Убираем требование градиента:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:24:04.770976Z",
          "start_time": "2019-11-18T17:24:04.766810Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psXlmjnx07OO",
        "outputId": "eb0dbdcc-3845-405a-b906-526ee7e07a72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=1000, bias=True)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:24:12.305790Z",
          "start_time": "2019-11-18T17:24:12.302517Z"
        },
        "id": "n637z2fz07OO"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(in_features=512, out_features=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:24:42.228326Z",
          "start_time": "2019-11-18T17:24:42.222643Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzYUzBDH07OP",
        "outputId": "b0fa069a-d57a-420d-a264-8e4a092d50ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "print(\"Params to learn:\")\n",
        "params_to_update = []\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:25:11.558131Z",
          "start_time": "2019-11-18T17:25:11.554358Z"
        },
        "id": "UEWnyfQD07OP"
      },
      "outputs": [],
      "source": [
        "trainer = torch.optim.Adam(params_to_update, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:26:20.718099Z",
          "start_time": "2019-11-18T17:25:20.979097Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzy-TnEs07OP",
        "outputId": "48cc2374-cc2b-4233-8d06-f38b6772268b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0. time since epoch: 0.458. Train acc: 0.099. Train Loss: 2.436\n",
            "Step 10. time since epoch: 3.373. Train acc: 0.264. Train Loss: 2.121\n",
            "Step 20. time since epoch: 6.383. Train acc: 0.455. Train Loss: 1.851\n",
            "Step 30. time since epoch: 9.295. Train acc: 0.561. Train Loss: 1.639\n",
            "Step 40. time since epoch: 12.272. Train acc: 0.626. Train Loss: 1.475\n",
            "Step 50. time since epoch: 15.201. Train acc: 0.671. Train Loss: 1.346\n",
            "Step 60. time since epoch: 18.116. Train acc: 0.705. Train Loss: 1.235\n",
            "Step 70. time since epoch: 20.999. Train acc: 0.731. Train Loss: 1.150\n",
            "Step 80. time since epoch: 23.889. Train acc: 0.752. Train Loss: 1.078\n",
            "Step 90. time since epoch: 26.803. Train acc: 0.767. Train Loss: 1.016\n",
            "Step 100. time since epoch: 29.707. Train acc: 0.781. Train Loss: 0.961\n",
            "Step 110. time since epoch: 32.644. Train acc: 0.793. Train Loss: 0.914\n",
            "Step 120. time since epoch: 35.551. Train acc: 0.803. Train Loss: 0.873\n",
            "Step 130. time since epoch: 38.456. Train acc: 0.812. Train Loss: 0.837\n",
            "Step 140. time since epoch: 41.368. Train acc: 0.820. Train Loss: 0.805\n",
            "Step 150. time since epoch: 44.569. Train acc: 0.825. Train Loss: 0.778\n",
            "Step 160. time since epoch: 47.501. Train acc: 0.831. Train Loss: 0.753\n",
            "Step 170. time since epoch: 50.413. Train acc: 0.835. Train Loss: 0.731\n",
            "Step 180. time since epoch: 53.331. Train acc: 0.841. Train Loss: 0.708\n",
            "Step 190. time since epoch: 56.229. Train acc: 0.845. Train Loss: 0.688\n",
            "Step 200. time since epoch: 59.140. Train acc: 0.848. Train Loss: 0.670\n",
            "Step 210. time since epoch: 62.025. Train acc: 0.852. Train Loss: 0.653\n",
            "Step 220. time since epoch: 65.034. Train acc: 0.856. Train Loss: 0.637\n",
            "Step 230. time since epoch: 68.151. Train acc: 0.859. Train Loss: 0.622\n",
            "Step 240. time since epoch: 71.366. Train acc: 0.862. Train Loss: 0.608\n",
            "Step 250. time since epoch: 74.507. Train acc: 0.865. Train Loss: 0.595\n",
            "Step 260. time since epoch: 77.670. Train acc: 0.867. Train Loss: 0.584\n",
            "Step 270. time since epoch: 80.695. Train acc: 0.870. Train Loss: 0.571\n",
            "Step 280. time since epoch: 83.611. Train acc: 0.872. Train Loss: 0.560\n",
            "Step 290. time since epoch: 86.500. Train acc: 0.875. Train Loss: 0.549\n",
            "Step 300. time since epoch: 89.399. Train acc: 0.877. Train Loss: 0.538\n",
            "Step 310. time since epoch: 92.290. Train acc: 0.879. Train Loss: 0.528\n",
            "--------------------\n",
            "epoch 1, loss 0.5264, train acc 0.880, test acc 0.940, time 105.3 sec\n",
            "Step 0. time since epoch: 0.308. Train acc: 0.938. Train Loss: 0.227\n",
            "Step 10. time since epoch: 3.202. Train acc: 0.939. Train Loss: 0.242\n",
            "Step 20. time since epoch: 6.227. Train acc: 0.948. Train Loss: 0.223\n",
            "Step 30. time since epoch: 9.171. Train acc: 0.946. Train Loss: 0.225\n",
            "Step 40. time since epoch: 12.085. Train acc: 0.945. Train Loss: 0.225\n",
            "Step 50. time since epoch: 15.078. Train acc: 0.943. Train Loss: 0.229\n",
            "Step 60. time since epoch: 17.958. Train acc: 0.943. Train Loss: 0.228\n",
            "Step 70. time since epoch: 20.973. Train acc: 0.943. Train Loss: 0.229\n",
            "Step 80. time since epoch: 23.865. Train acc: 0.943. Train Loss: 0.228\n",
            "Step 90. time since epoch: 26.949. Train acc: 0.943. Train Loss: 0.227\n",
            "Step 100. time since epoch: 29.855. Train acc: 0.943. Train Loss: 0.224\n",
            "Step 110. time since epoch: 32.864. Train acc: 0.944. Train Loss: 0.221\n",
            "Step 120. time since epoch: 35.865. Train acc: 0.945. Train Loss: 0.220\n",
            "Step 130. time since epoch: 38.871. Train acc: 0.945. Train Loss: 0.219\n",
            "Step 140. time since epoch: 41.843. Train acc: 0.945. Train Loss: 0.218\n",
            "Step 150. time since epoch: 44.799. Train acc: 0.944. Train Loss: 0.218\n",
            "Step 160. time since epoch: 47.684. Train acc: 0.944. Train Loss: 0.217\n",
            "Step 170. time since epoch: 50.657. Train acc: 0.943. Train Loss: 0.218\n",
            "Step 180. time since epoch: 53.607. Train acc: 0.944. Train Loss: 0.216\n",
            "Step 190. time since epoch: 56.514. Train acc: 0.944. Train Loss: 0.215\n",
            "Step 200. time since epoch: 59.417. Train acc: 0.945. Train Loss: 0.214\n",
            "Step 210. time since epoch: 62.348. Train acc: 0.945. Train Loss: 0.213\n",
            "Step 220. time since epoch: 65.556. Train acc: 0.945. Train Loss: 0.211\n",
            "Step 230. time since epoch: 68.523. Train acc: 0.946. Train Loss: 0.210\n",
            "Step 240. time since epoch: 71.432. Train acc: 0.946. Train Loss: 0.209\n",
            "Step 250. time since epoch: 74.327. Train acc: 0.946. Train Loss: 0.209\n",
            "Step 260. time since epoch: 77.332. Train acc: 0.945. Train Loss: 0.209\n",
            "Step 270. time since epoch: 80.235. Train acc: 0.946. Train Loss: 0.207\n",
            "Step 280. time since epoch: 83.120. Train acc: 0.946. Train Loss: 0.206\n",
            "Step 290. time since epoch: 86.018. Train acc: 0.946. Train Loss: 0.205\n",
            "Step 300. time since epoch: 89.156. Train acc: 0.947. Train Loss: 0.203\n",
            "Step 310. time since epoch: 92.076. Train acc: 0.947. Train Loss: 0.201\n",
            "--------------------\n",
            "epoch 2, loss 0.2008, train acc 0.947, test acc 0.954, time 104.9 sec\n",
            "Step 0. time since epoch: 0.282. Train acc: 0.948. Train Loss: 0.172\n",
            "Step 10. time since epoch: 3.191. Train acc: 0.951. Train Loss: 0.175\n",
            "Step 20. time since epoch: 6.204. Train acc: 0.957. Train Loss: 0.160\n",
            "Step 30. time since epoch: 9.206. Train acc: 0.956. Train Loss: 0.162\n",
            "Step 40. time since epoch: 12.145. Train acc: 0.956. Train Loss: 0.163\n",
            "Step 50. time since epoch: 15.104. Train acc: 0.955. Train Loss: 0.167\n",
            "Step 60. time since epoch: 18.046. Train acc: 0.954. Train Loss: 0.167\n",
            "Step 70. time since epoch: 20.952. Train acc: 0.954. Train Loss: 0.169\n",
            "Step 80. time since epoch: 23.950. Train acc: 0.954. Train Loss: 0.168\n",
            "Step 90. time since epoch: 26.981. Train acc: 0.954. Train Loss: 0.168\n",
            "Step 100. time since epoch: 29.896. Train acc: 0.955. Train Loss: 0.167\n",
            "Step 110. time since epoch: 32.891. Train acc: 0.955. Train Loss: 0.164\n",
            "Step 120. time since epoch: 35.911. Train acc: 0.956. Train Loss: 0.163\n",
            "Step 130. time since epoch: 38.930. Train acc: 0.955. Train Loss: 0.163\n",
            "Step 140. time since epoch: 41.944. Train acc: 0.955. Train Loss: 0.163\n",
            "Step 150. time since epoch: 44.916. Train acc: 0.955. Train Loss: 0.164\n",
            "Step 160. time since epoch: 47.945. Train acc: 0.955. Train Loss: 0.164\n",
            "Step 170. time since epoch: 51.032. Train acc: 0.954. Train Loss: 0.165\n",
            "Step 180. time since epoch: 54.251. Train acc: 0.955. Train Loss: 0.164\n",
            "Step 190. time since epoch: 57.233. Train acc: 0.955. Train Loss: 0.163\n",
            "Step 200. time since epoch: 60.197. Train acc: 0.955. Train Loss: 0.163\n",
            "Step 210. time since epoch: 63.094. Train acc: 0.955. Train Loss: 0.162\n",
            "Step 220. time since epoch: 65.994. Train acc: 0.955. Train Loss: 0.162\n",
            "Step 230. time since epoch: 68.926. Train acc: 0.955. Train Loss: 0.161\n",
            "Step 240. time since epoch: 71.907. Train acc: 0.955. Train Loss: 0.161\n",
            "Step 250. time since epoch: 74.903. Train acc: 0.955. Train Loss: 0.161\n",
            "Step 260. time since epoch: 77.829. Train acc: 0.955. Train Loss: 0.162\n",
            "Step 270. time since epoch: 80.800. Train acc: 0.955. Train Loss: 0.161\n",
            "Step 280. time since epoch: 83.692. Train acc: 0.955. Train Loss: 0.160\n",
            "Step 290. time since epoch: 86.643. Train acc: 0.955. Train Loss: 0.159\n",
            "Step 300. time since epoch: 89.640. Train acc: 0.956. Train Loss: 0.158\n",
            "Step 310. time since epoch: 92.595. Train acc: 0.956. Train Loss: 0.157\n",
            "--------------------\n",
            "epoch 3, loss 0.1569, train acc 0.956, test acc 0.960, time 105.5 sec\n",
            "Step 0. time since epoch: 0.279. Train acc: 0.958. Train Loss: 0.150\n",
            "Step 10. time since epoch: 3.179. Train acc: 0.956. Train Loss: 0.148\n",
            "Step 20. time since epoch: 6.116. Train acc: 0.963. Train Loss: 0.133\n",
            "Step 30. time since epoch: 9.032. Train acc: 0.962. Train Loss: 0.136\n",
            "Step 40. time since epoch: 11.964. Train acc: 0.962. Train Loss: 0.137\n",
            "Step 50. time since epoch: 14.928. Train acc: 0.960. Train Loss: 0.141\n",
            "Step 60. time since epoch: 17.871. Train acc: 0.960. Train Loss: 0.141\n",
            "Step 70. time since epoch: 20.826. Train acc: 0.959. Train Loss: 0.143\n",
            "Step 80. time since epoch: 23.783. Train acc: 0.960. Train Loss: 0.143\n",
            "Step 90. time since epoch: 26.728. Train acc: 0.959. Train Loss: 0.143\n",
            "Step 100. time since epoch: 29.686. Train acc: 0.960. Train Loss: 0.141\n",
            "Step 110. time since epoch: 32.658. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 120. time since epoch: 35.637. Train acc: 0.961. Train Loss: 0.139\n",
            "Step 130. time since epoch: 38.617. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 140. time since epoch: 41.591. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 150. time since epoch: 44.570. Train acc: 0.960. Train Loss: 0.140\n",
            "Step 160. time since epoch: 47.531. Train acc: 0.960. Train Loss: 0.140\n",
            "Step 170. time since epoch: 50.551. Train acc: 0.960. Train Loss: 0.141\n",
            "Step 180. time since epoch: 53.512. Train acc: 0.960. Train Loss: 0.140\n",
            "Step 190. time since epoch: 56.481. Train acc: 0.960. Train Loss: 0.140\n",
            "Step 200. time since epoch: 59.525. Train acc: 0.960. Train Loss: 0.140\n",
            "Step 210. time since epoch: 62.493. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 220. time since epoch: 65.480. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 230. time since epoch: 68.439. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 240. time since epoch: 71.767. Train acc: 0.960. Train Loss: 0.138\n",
            "Step 250. time since epoch: 74.774. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 260. time since epoch: 77.694. Train acc: 0.959. Train Loss: 0.139\n",
            "Step 270. time since epoch: 80.681. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 280. time since epoch: 83.754. Train acc: 0.960. Train Loss: 0.139\n",
            "Step 290. time since epoch: 86.713. Train acc: 0.960. Train Loss: 0.138\n",
            "Step 300. time since epoch: 89.670. Train acc: 0.961. Train Loss: 0.137\n",
            "Step 310. time since epoch: 92.611. Train acc: 0.961. Train Loss: 0.136\n",
            "--------------------\n",
            "epoch 4, loss 0.1358, train acc 0.961, test acc 0.962, time 105.7 sec\n",
            "Step 0. time since epoch: 0.310. Train acc: 0.958. Train Loss: 0.139\n",
            "Step 10. time since epoch: 3.286. Train acc: 0.959. Train Loss: 0.133\n",
            "Step 20. time since epoch: 6.272. Train acc: 0.966. Train Loss: 0.118\n",
            "Step 30. time since epoch: 9.221. Train acc: 0.965. Train Loss: 0.121\n",
            "Step 40. time since epoch: 12.219. Train acc: 0.965. Train Loss: 0.122\n",
            "Step 50. time since epoch: 15.244. Train acc: 0.963. Train Loss: 0.126\n",
            "Step 60. time since epoch: 18.194. Train acc: 0.963. Train Loss: 0.126\n",
            "Step 70. time since epoch: 21.154. Train acc: 0.962. Train Loss: 0.128\n",
            "Step 80. time since epoch: 24.094. Train acc: 0.963. Train Loss: 0.127\n",
            "Step 90. time since epoch: 27.049. Train acc: 0.962. Train Loss: 0.127\n",
            "Step 100. time since epoch: 30.006. Train acc: 0.963. Train Loss: 0.126\n",
            "Step 110. time since epoch: 33.001. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 120. time since epoch: 36.088. Train acc: 0.964. Train Loss: 0.124\n",
            "Step 130. time since epoch: 39.075. Train acc: 0.964. Train Loss: 0.124\n",
            "Step 140. time since epoch: 42.027. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 150. time since epoch: 44.987. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 160. time since epoch: 48.007. Train acc: 0.963. Train Loss: 0.126\n",
            "Step 170. time since epoch: 50.948. Train acc: 0.963. Train Loss: 0.127\n",
            "Step 180. time since epoch: 53.948. Train acc: 0.963. Train Loss: 0.126\n",
            "Step 190. time since epoch: 56.941. Train acc: 0.963. Train Loss: 0.126\n",
            "Step 200. time since epoch: 59.909. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 210. time since epoch: 62.857. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 220. time since epoch: 65.816. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 230. time since epoch: 68.762. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 240. time since epoch: 71.709. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 250. time since epoch: 74.898. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 260. time since epoch: 77.923. Train acc: 0.963. Train Loss: 0.126\n",
            "Step 270. time since epoch: 80.936. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 280. time since epoch: 84.328. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 290. time since epoch: 87.273. Train acc: 0.963. Train Loss: 0.125\n",
            "Step 300. time since epoch: 90.253. Train acc: 0.964. Train Loss: 0.124\n",
            "Step 310. time since epoch: 93.217. Train acc: 0.964. Train Loss: 0.123\n",
            "--------------------\n",
            "epoch 5, loss 0.1228, train acc 0.964, test acc 0.962, time 106.3 sec\n",
            "Step 0. time since epoch: 0.282. Train acc: 0.958. Train Loss: 0.132\n",
            "Step 10. time since epoch: 3.257. Train acc: 0.962. Train Loss: 0.123\n",
            "Step 20. time since epoch: 6.237. Train acc: 0.968. Train Loss: 0.108\n",
            "Step 30. time since epoch: 9.169. Train acc: 0.967. Train Loss: 0.111\n",
            "Step 40. time since epoch: 12.139. Train acc: 0.966. Train Loss: 0.112\n",
            "Step 50. time since epoch: 15.200. Train acc: 0.965. Train Loss: 0.116\n",
            "Step 60. time since epoch: 18.157. Train acc: 0.965. Train Loss: 0.115\n",
            "Step 70. time since epoch: 21.216. Train acc: 0.964. Train Loss: 0.117\n",
            "Step 80. time since epoch: 24.180. Train acc: 0.965. Train Loss: 0.117\n",
            "Step 90. time since epoch: 27.202. Train acc: 0.965. Train Loss: 0.117\n",
            "Step 100. time since epoch: 30.247. Train acc: 0.965. Train Loss: 0.116\n",
            "Step 110. time since epoch: 33.197. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 120. time since epoch: 36.175. Train acc: 0.966. Train Loss: 0.114\n",
            "Step 130. time since epoch: 39.137. Train acc: 0.966. Train Loss: 0.114\n",
            "Step 140. time since epoch: 42.139. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 150. time since epoch: 45.165. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 160. time since epoch: 48.165. Train acc: 0.966. Train Loss: 0.116\n",
            "Step 170. time since epoch: 51.169. Train acc: 0.965. Train Loss: 0.117\n",
            "Step 180. time since epoch: 54.304. Train acc: 0.965. Train Loss: 0.116\n",
            "Step 190. time since epoch: 57.467. Train acc: 0.966. Train Loss: 0.116\n",
            "Step 200. time since epoch: 60.458. Train acc: 0.966. Train Loss: 0.116\n",
            "Step 210. time since epoch: 63.689. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 220. time since epoch: 66.697. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 230. time since epoch: 69.703. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 240. time since epoch: 72.682. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 250. time since epoch: 75.685. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 260. time since epoch: 78.679. Train acc: 0.965. Train Loss: 0.116\n",
            "Step 270. time since epoch: 81.676. Train acc: 0.965. Train Loss: 0.116\n",
            "Step 280. time since epoch: 84.695. Train acc: 0.965. Train Loss: 0.116\n",
            "Step 290. time since epoch: 87.656. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 300. time since epoch: 90.746. Train acc: 0.966. Train Loss: 0.115\n",
            "Step 310. time since epoch: 93.797. Train acc: 0.966. Train Loss: 0.114\n",
            "--------------------\n",
            "epoch 6, loss 0.1137, train acc 0.966, test acc 0.963, time 106.5 sec\n",
            "Step 0. time since epoch: 0.289. Train acc: 0.958. Train Loss: 0.127\n",
            "Step 10. time since epoch: 3.237. Train acc: 0.964. Train Loss: 0.115\n",
            "Step 20. time since epoch: 6.199. Train acc: 0.970. Train Loss: 0.100\n",
            "Step 30. time since epoch: 9.675. Train acc: 0.969. Train Loss: 0.103\n",
            "Step 40. time since epoch: 12.655. Train acc: 0.968. Train Loss: 0.104\n",
            "Step 50. time since epoch: 16.184. Train acc: 0.967. Train Loss: 0.109\n",
            "Step 60. time since epoch: 19.198. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 70. time since epoch: 22.144. Train acc: 0.966. Train Loss: 0.110\n",
            "Step 80. time since epoch: 25.127. Train acc: 0.967. Train Loss: 0.110\n",
            "Step 90. time since epoch: 28.097. Train acc: 0.967. Train Loss: 0.110\n",
            "Step 100. time since epoch: 31.079. Train acc: 0.967. Train Loss: 0.109\n",
            "Step 110. time since epoch: 34.045. Train acc: 0.967. Train Loss: 0.107\n",
            "Step 120. time since epoch: 37.007. Train acc: 0.968. Train Loss: 0.107\n",
            "Step 130. time since epoch: 39.978. Train acc: 0.968. Train Loss: 0.107\n",
            "Step 140. time since epoch: 42.963. Train acc: 0.967. Train Loss: 0.107\n",
            "Step 150. time since epoch: 45.976. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 160. time since epoch: 48.920. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 170. time since epoch: 51.969. Train acc: 0.967. Train Loss: 0.110\n",
            "Step 180. time since epoch: 54.934. Train acc: 0.967. Train Loss: 0.109\n",
            "Step 190. time since epoch: 57.902. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 200. time since epoch: 60.862. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 210. time since epoch: 63.938. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 220. time since epoch: 67.009. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 230. time since epoch: 69.970. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 240. time since epoch: 73.006. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 250. time since epoch: 75.979. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 260. time since epoch: 79.037. Train acc: 0.967. Train Loss: 0.109\n",
            "Step 270. time since epoch: 82.018. Train acc: 0.967. Train Loss: 0.109\n",
            "Step 280. time since epoch: 84.988. Train acc: 0.967. Train Loss: 0.109\n",
            "Step 290. time since epoch: 87.956. Train acc: 0.967. Train Loss: 0.108\n",
            "Step 300. time since epoch: 90.914. Train acc: 0.968. Train Loss: 0.108\n",
            "Step 310. time since epoch: 93.882. Train acc: 0.968. Train Loss: 0.107\n",
            "--------------------\n",
            "epoch 7, loss 0.1068, train acc 0.968, test acc 0.964, time 106.6 sec\n",
            "Step 0. time since epoch: 0.291. Train acc: 0.958. Train Loss: 0.124\n",
            "Step 10. time since epoch: 3.245. Train acc: 0.966. Train Loss: 0.109\n",
            "Step 20. time since epoch: 6.260. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 30. time since epoch: 9.255. Train acc: 0.970. Train Loss: 0.097\n",
            "Step 40. time since epoch: 12.222. Train acc: 0.970. Train Loss: 0.099\n",
            "Step 50. time since epoch: 15.201. Train acc: 0.969. Train Loss: 0.103\n",
            "Step 60. time since epoch: 18.156. Train acc: 0.969. Train Loss: 0.102\n",
            "Step 70. time since epoch: 21.114. Train acc: 0.968. Train Loss: 0.104\n",
            "Step 80. time since epoch: 24.086. Train acc: 0.969. Train Loss: 0.104\n",
            "Step 90. time since epoch: 27.070. Train acc: 0.968. Train Loss: 0.104\n",
            "Step 100. time since epoch: 30.039. Train acc: 0.968. Train Loss: 0.103\n",
            "Step 110. time since epoch: 33.339. Train acc: 0.969. Train Loss: 0.101\n",
            "Step 120. time since epoch: 36.319. Train acc: 0.969. Train Loss: 0.101\n",
            "Step 130. time since epoch: 39.305. Train acc: 0.969. Train Loss: 0.101\n",
            "Step 140. time since epoch: 42.261. Train acc: 0.969. Train Loss: 0.101\n",
            "Step 150. time since epoch: 45.217. Train acc: 0.969. Train Loss: 0.102\n",
            "Step 160. time since epoch: 48.200. Train acc: 0.969. Train Loss: 0.103\n",
            "Step 170. time since epoch: 51.128. Train acc: 0.968. Train Loss: 0.104\n",
            "Step 180. time since epoch: 54.198. Train acc: 0.968. Train Loss: 0.103\n",
            "Step 190. time since epoch: 57.153. Train acc: 0.969. Train Loss: 0.103\n",
            "Step 200. time since epoch: 60.183. Train acc: 0.969. Train Loss: 0.102\n",
            "Step 210. time since epoch: 63.153. Train acc: 0.969. Train Loss: 0.103\n",
            "Step 220. time since epoch: 66.172. Train acc: 0.969. Train Loss: 0.102\n",
            "Step 230. time since epoch: 69.138. Train acc: 0.969. Train Loss: 0.102\n",
            "Step 240. time since epoch: 72.099. Train acc: 0.969. Train Loss: 0.102\n",
            "Step 250. time since epoch: 75.076. Train acc: 0.969. Train Loss: 0.103\n",
            "Step 260. time since epoch: 78.045. Train acc: 0.968. Train Loss: 0.104\n",
            "Step 270. time since epoch: 81.035. Train acc: 0.969. Train Loss: 0.104\n",
            "Step 280. time since epoch: 84.004. Train acc: 0.969. Train Loss: 0.104\n",
            "Step 290. time since epoch: 86.973. Train acc: 0.969. Train Loss: 0.103\n",
            "Step 300. time since epoch: 89.944. Train acc: 0.969. Train Loss: 0.102\n",
            "Step 310. time since epoch: 92.905. Train acc: 0.969. Train Loss: 0.101\n",
            "--------------------\n",
            "epoch 8, loss 0.1014, train acc 0.969, test acc 0.965, time 105.5 sec\n",
            "Step 0. time since epoch: 0.278. Train acc: 0.958. Train Loss: 0.121\n",
            "Step 10. time since epoch: 3.192. Train acc: 0.966. Train Loss: 0.105\n",
            "Step 20. time since epoch: 6.150. Train acc: 0.973. Train Loss: 0.090\n",
            "Step 30. time since epoch: 9.134. Train acc: 0.971. Train Loss: 0.092\n",
            "Step 40. time since epoch: 12.091. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 50. time since epoch: 15.114. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 60. time since epoch: 18.347. Train acc: 0.970. Train Loss: 0.097\n",
            "Step 70. time since epoch: 21.290. Train acc: 0.969. Train Loss: 0.099\n",
            "Step 80. time since epoch: 24.331. Train acc: 0.970. Train Loss: 0.099\n",
            "Step 90. time since epoch: 27.283. Train acc: 0.970. Train Loss: 0.099\n",
            "Step 100. time since epoch: 30.350. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 110. time since epoch: 33.340. Train acc: 0.970. Train Loss: 0.097\n",
            "Step 120. time since epoch: 36.336. Train acc: 0.970. Train Loss: 0.096\n",
            "Step 130. time since epoch: 39.297. Train acc: 0.970. Train Loss: 0.096\n",
            "Step 140. time since epoch: 42.274. Train acc: 0.970. Train Loss: 0.097\n",
            "Step 150. time since epoch: 45.336. Train acc: 0.970. Train Loss: 0.097\n",
            "Step 160. time since epoch: 48.300. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 170. time since epoch: 51.299. Train acc: 0.970. Train Loss: 0.099\n",
            "Step 180. time since epoch: 54.242. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 190. time since epoch: 57.198. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 200. time since epoch: 60.173. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 210. time since epoch: 63.150. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 220. time since epoch: 66.106. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 230. time since epoch: 69.077. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 240. time since epoch: 72.046. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 250. time since epoch: 75.096. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 260. time since epoch: 78.070. Train acc: 0.970. Train Loss: 0.099\n",
            "Step 270. time since epoch: 81.045. Train acc: 0.970. Train Loss: 0.099\n",
            "Step 280. time since epoch: 84.008. Train acc: 0.970. Train Loss: 0.099\n",
            "Step 290. time since epoch: 87.001. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 300. time since epoch: 89.973. Train acc: 0.970. Train Loss: 0.098\n",
            "Step 310. time since epoch: 92.942. Train acc: 0.971. Train Loss: 0.097\n",
            "--------------------\n",
            "epoch 9, loss 0.0970, train acc 0.971, test acc 0.965, time 105.6 sec\n",
            "Step 0. time since epoch: 0.307. Train acc: 0.964. Train Loss: 0.118\n",
            "Step 10. time since epoch: 3.334. Train acc: 0.967. Train Loss: 0.101\n",
            "Step 20. time since epoch: 6.343. Train acc: 0.974. Train Loss: 0.086\n",
            "Step 30. time since epoch: 9.406. Train acc: 0.972. Train Loss: 0.088\n",
            "Step 40. time since epoch: 12.391. Train acc: 0.972. Train Loss: 0.090\n",
            "Step 50. time since epoch: 15.346. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 60. time since epoch: 18.423. Train acc: 0.971. Train Loss: 0.093\n",
            "Step 70. time since epoch: 21.390. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 80. time since epoch: 24.356. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 90. time since epoch: 27.309. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 100. time since epoch: 30.319. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 110. time since epoch: 33.653. Train acc: 0.971. Train Loss: 0.093\n",
            "Step 120. time since epoch: 36.661. Train acc: 0.971. Train Loss: 0.092\n",
            "Step 130. time since epoch: 39.644. Train acc: 0.971. Train Loss: 0.092\n",
            "Step 140. time since epoch: 42.618. Train acc: 0.971. Train Loss: 0.093\n",
            "Step 150. time since epoch: 45.669. Train acc: 0.971. Train Loss: 0.093\n",
            "Step 160. time since epoch: 48.671. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 170. time since epoch: 51.631. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 180. time since epoch: 54.639. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 190. time since epoch: 57.616. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 200. time since epoch: 60.569. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 210. time since epoch: 63.543. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 220. time since epoch: 66.508. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 230. time since epoch: 69.473. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 240. time since epoch: 72.442. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 250. time since epoch: 75.387. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 260. time since epoch: 78.356. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 270. time since epoch: 81.317. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 280. time since epoch: 84.266. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 290. time since epoch: 87.321. Train acc: 0.971. Train Loss: 0.095\n",
            "Step 300. time since epoch: 90.341. Train acc: 0.971. Train Loss: 0.094\n",
            "Step 310. time since epoch: 93.334. Train acc: 0.972. Train Loss: 0.093\n",
            "--------------------\n",
            "epoch 10, loss 0.0933, train acc 0.972, test acc 0.966, time 105.9 sec\n"
          ]
        }
      ],
      "source": [
        "train_loss_resnet18 = train(model, train_iter, test_iter, trainer, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz6qmsROAGcq"
      },
      "source": [
        "#### vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# VGG11\n",
        "model_vgg16 = tv.models.vgg16(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-Mfy6xdAGmq",
        "outputId": "b849d21e-7ac0-4416-ee31-9c9fa3d8c752"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:23:47.319979Z",
          "start_time": "2019-11-18T17:23:47.316747Z"
        },
        "id": "g7d8QjgYAZ_g"
      },
      "outputs": [],
      "source": [
        "# Убираем требование градиента:\n",
        "for param in model_vgg16.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:24:04.770976Z",
          "start_time": "2019-11-18T17:24:04.766810Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgexjk8JAZ_j",
        "outputId": "3e9aab48-7962-4898-a5bc-853a990d9391"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_vgg16.classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:24:12.305790Z",
          "start_time": "2019-11-18T17:24:12.302517Z"
        },
        "id": "uSzRrmxVAZ_l"
      },
      "outputs": [],
      "source": [
        "model_vgg16.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096), \n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(4096, 10)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t classifier.0.weight\n",
            "\t classifier.0.bias\n",
            "\t classifier.3.weight\n",
            "\t classifier.3.bias\n"
          ]
        }
      ],
      "source": [
        "# Распечатать заголовок секции \n",
        "print(\"Params to learn:\")  \n",
        "\n",
        "# Создать пустой список для хранения обучаемых параметров\n",
        "params_to_update = []  \n",
        "\n",
        "# Получить именованные параметры модели в виде списка tuple\n",
        "for name, param in model_vgg16.named_parameters():\n",
        "    \n",
        "    # Проверить флаг requires_grad, который указывает нужно ли обучать этот параметр\n",
        "    if param.requires_grad == True:\n",
        "\n",
        "        # Добавить параметр в список обучаемых\n",
        "        params_to_update.append(param)\n",
        "        \n",
        "        # Распечатать имя параметра с отступом\n",
        "        print(\"\\t\",name)\n",
        "        \n",
        "# Теперь в params_to_update будут только обучаемые параметры модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:25:11.558131Z",
          "start_time": "2019-11-18T17:25:11.554358Z"
        },
        "id": "wJqgDnKIAZ_o"
      },
      "outputs": [],
      "source": [
        "trainer = torch.optim.Adam(params_to_update, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-18T17:26:20.718099Z",
          "start_time": "2019-11-18T17:25:20.979097Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqUMukocAZ_o",
        "outputId": "a0788b73-0de1-4542-fd69-c577a3d1c30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0. time since epoch: 0.860. Train acc: 0.089. Train Loss: 2.304\n",
            "Step 10. time since epoch: 49.124. Train acc: 0.718. Train Loss: 1.022\n",
            "Step 20. time since epoch: 96.388. Train acc: 0.829. Train Loss: 0.623\n",
            "Step 30. time since epoch: 144.032. Train acc: 0.871. Train Loss: 0.482\n",
            "Step 40. time since epoch: 191.301. Train acc: 0.896. Train Loss: 0.388\n",
            "Step 50. time since epoch: 238.515. Train acc: 0.910. Train Loss: 0.332\n",
            "Step 60. time since epoch: 285.805. Train acc: 0.922. Train Loss: 0.288\n",
            "Step 70. time since epoch: 333.019. Train acc: 0.930. Train Loss: 0.257\n",
            "Step 80. time since epoch: 380.258. Train acc: 0.936. Train Loss: 0.234\n",
            "Step 90. time since epoch: 427.460. Train acc: 0.940. Train Loss: 0.216\n",
            "Step 100. time since epoch: 474.661. Train acc: 0.944. Train Loss: 0.202\n",
            "Step 110. time since epoch: 521.867. Train acc: 0.948. Train Loss: 0.187\n",
            "Step 120. time since epoch: 569.143. Train acc: 0.951. Train Loss: 0.175\n",
            "Step 130. time since epoch: 616.353. Train acc: 0.954. Train Loss: 0.166\n",
            "Step 140. time since epoch: 663.548. Train acc: 0.956. Train Loss: 0.160\n",
            "Step 150. time since epoch: 710.762. Train acc: 0.957. Train Loss: 0.153\n",
            "Step 160. time since epoch: 758.166. Train acc: 0.959. Train Loss: 0.147\n",
            "Step 170. time since epoch: 805.410. Train acc: 0.959. Train Loss: 0.145\n",
            "Step 180. time since epoch: 852.659. Train acc: 0.960. Train Loss: 0.141\n",
            "Step 190. time since epoch: 899.840. Train acc: 0.962. Train Loss: 0.138\n",
            "Step 200. time since epoch: 947.225. Train acc: 0.962. Train Loss: 0.135\n",
            "Step 210. time since epoch: 994.482. Train acc: 0.963. Train Loss: 0.133\n",
            "Step 220. time since epoch: 1041.708. Train acc: 0.963. Train Loss: 0.132\n",
            "Step 230. time since epoch: 1089.510. Train acc: 0.963. Train Loss: 0.131\n",
            "Step 240. time since epoch: 1136.728. Train acc: 0.964. Train Loss: 0.129\n",
            "Step 250. time since epoch: 1183.917. Train acc: 0.965. Train Loss: 0.127\n",
            "Step 260. time since epoch: 1231.092. Train acc: 0.965. Train Loss: 0.125\n",
            "Step 270. time since epoch: 1278.433. Train acc: 0.965. Train Loss: 0.123\n",
            "Step 280. time since epoch: 1325.652. Train acc: 0.966. Train Loss: 0.121\n",
            "Step 290. time since epoch: 1372.940. Train acc: 0.967. Train Loss: 0.119\n",
            "Step 300. time since epoch: 1420.214. Train acc: 0.967. Train Loss: 0.116\n",
            "Step 310. time since epoch: 1467.494. Train acc: 0.968. Train Loss: 0.114\n",
            "--------------------\n",
            "epoch 1, loss 0.1145, train acc 0.968, test acc 0.949, time 1709.0 sec\n",
            "Step 0. time since epoch: 4.764. Train acc: 0.979. Train Loss: 0.063\n",
            "Step 10. time since epoch: 52.085. Train acc: 0.980. Train Loss: 0.075\n",
            "Step 20. time since epoch: 99.364. Train acc: 0.985. Train Loss: 0.057\n",
            "Step 30. time since epoch: 146.636. Train acc: 0.985. Train Loss: 0.055\n",
            "Step 40. time since epoch: 193.868. Train acc: 0.986. Train Loss: 0.051\n",
            "Step 50. time since epoch: 241.108. Train acc: 0.987. Train Loss: 0.047\n",
            "Step 60. time since epoch: 288.272. Train acc: 0.988. Train Loss: 0.044\n",
            "Step 70. time since epoch: 335.419. Train acc: 0.989. Train Loss: 0.041\n",
            "Step 80. time since epoch: 382.580. Train acc: 0.989. Train Loss: 0.039\n",
            "Step 90. time since epoch: 429.792. Train acc: 0.990. Train Loss: 0.037\n",
            "Step 100. time since epoch: 477.053. Train acc: 0.990. Train Loss: 0.035\n",
            "Step 110. time since epoch: 524.338. Train acc: 0.990. Train Loss: 0.033\n",
            "Step 120. time since epoch: 571.530. Train acc: 0.991. Train Loss: 0.032\n",
            "Step 130. time since epoch: 618.675. Train acc: 0.991. Train Loss: 0.031\n",
            "Step 140. time since epoch: 666.029. Train acc: 0.991. Train Loss: 0.031\n",
            "Step 150. time since epoch: 713.260. Train acc: 0.991. Train Loss: 0.030\n",
            "Step 160. time since epoch: 760.472. Train acc: 0.991. Train Loss: 0.029\n",
            "Step 170. time since epoch: 807.709. Train acc: 0.991. Train Loss: 0.030\n",
            "Step 180. time since epoch: 854.946. Train acc: 0.991. Train Loss: 0.030\n",
            "Step 190. time since epoch: 902.125. Train acc: 0.991. Train Loss: 0.030\n",
            "Step 200. time since epoch: 949.320. Train acc: 0.991. Train Loss: 0.030\n",
            "Step 210. time since epoch: 996.528. Train acc: 0.992. Train Loss: 0.030\n",
            "Step 220. time since epoch: 1043.859. Train acc: 0.992. Train Loss: 0.029\n",
            "Step 230. time since epoch: 1091.050. Train acc: 0.992. Train Loss: 0.029\n",
            "Step 240. time since epoch: 1138.221. Train acc: 0.992. Train Loss: 0.029\n",
            "Step 250. time since epoch: 1185.389. Train acc: 0.992. Train Loss: 0.029\n",
            "Step 260. time since epoch: 1232.631. Train acc: 0.992. Train Loss: 0.028\n",
            "Step 270. time since epoch: 1280.303. Train acc: 0.992. Train Loss: 0.028\n",
            "Step 280. time since epoch: 1327.509. Train acc: 0.992. Train Loss: 0.028\n",
            "Step 290. time since epoch: 1374.948. Train acc: 0.992. Train Loss: 0.028\n",
            "Step 300. time since epoch: 1422.297. Train acc: 0.992. Train Loss: 0.028\n",
            "Step 310. time since epoch: 1469.543. Train acc: 0.992. Train Loss: 0.027\n",
            "--------------------\n",
            "epoch 2, loss 0.0279, train acc 0.992, test acc 0.986, time 1710.9 sec\n",
            "Step 0. time since epoch: 4.779. Train acc: 0.990. Train Loss: 0.034\n",
            "Step 10. time since epoch: 51.994. Train acc: 0.991. Train Loss: 0.028\n",
            "Step 20. time since epoch: 99.184. Train acc: 0.993. Train Loss: 0.023\n",
            "Step 30. time since epoch: 146.444. Train acc: 0.995. Train Loss: 0.018\n",
            "Step 40. time since epoch: 193.594. Train acc: 0.995. Train Loss: 0.017\n",
            "Step 50. time since epoch: 240.783. Train acc: 0.995. Train Loss: 0.016\n",
            "Step 60. time since epoch: 288.025. Train acc: 0.996. Train Loss: 0.015\n",
            "Step 70. time since epoch: 335.325. Train acc: 0.996. Train Loss: 0.014\n",
            "Step 80. time since epoch: 382.510. Train acc: 0.996. Train Loss: 0.013\n",
            "Step 90. time since epoch: 429.686. Train acc: 0.996. Train Loss: 0.012\n",
            "Step 100. time since epoch: 476.839. Train acc: 0.996. Train Loss: 0.012\n",
            "Step 110. time since epoch: 524.086. Train acc: 0.997. Train Loss: 0.011\n",
            "Step 120. time since epoch: 571.473. Train acc: 0.997. Train Loss: 0.011\n"
          ]
        }
      ],
      "source": [
        "train_loss_vgg16 = train(model_vgg16, train_iter, test_iter, trainer, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### densenet161"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tv.models.densenet161(pretrained=True)\n",
        "# Убираем требование градиента:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.classifier = nn.Linear(in_features=2208, out_features=10)\n",
        "params_to_update = []\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "trainer = torch.optim.Adam(params_to_update, lr=0.001)\n",
        "train_loss_densenet161=train(model, train_iter, test_iter, trainer, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inception v3 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 192\n",
        "transoforms = tv.transforms.Compose([\n",
        "    tv.transforms.Grayscale(3),\n",
        "    tv.transforms.Resize((299, 299)),\n",
        "    tv.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = tv.datasets.EMNIST('.', train=True, transform=transoforms, download=True, split='mnist')\n",
        "test_dataset = tv.datasets.EMNIST('.', train=False, transform=transoforms, download=True, split='mnist')\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tv.models.inception_v3(pretrained=True)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0, 0\n",
        "    net.eval()\n",
        "    for X, y in data_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat = net(X)\n",
        "        acc_sum += (y_hat.argmax(axis=1) == y).sum()\n",
        "        n += y.shape[0]\n",
        "        return 0\n",
        "    return acc_sum.item() / n\n",
        "\n",
        "\n",
        "def train(net, train_iter, test_iter, trainer, num_epochs):\n",
        "    train_loss = []\n",
        "    net.to(device)\n",
        "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
        "    #net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        net.train()\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            trainer.zero_grad()\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat[0], y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat[0].argmax(axis=1) == y).sum().item()\n",
        "            n += y.shape[0]\n",
        "\n",
        "            if i % 100 == 0:\n",
        "              print(f\"Step {i}. time since epoch: {time.time() -  start:.3f}. \"\n",
        "                    f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
        "        test_acc = evaluate_accuracy(test_iter, net.to(device))\n",
        "        print('-' * 20)\n",
        "        train_loss.append(train_l_sum / n)\n",
        "        print(f'epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}'\n",
        "              f', test acc {test_acc:.3f}, time {time.time() - start:.1f} sec')\n",
        "    return train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Убираем требование градиента:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.fc = nn.Linear(in_features=2048, out_features=10)\n",
        "params_to_update = []\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)\n",
        "trainer = torch.optim.Adam(params_to_update, lr=0.001)\n",
        "train_loss_inception_v3=train(model, train_iter, test_iter, trainer, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['ResNet18', 'VGG16', 'DenseNet161', 'InceptionV3']\n",
        "train_losses = [train_loss_resnet18, train_loss_vgg16, train_loss_densenet161, train_loss_inception_v3]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "for i, loss in enumerate(train_losses):\n",
        "  plt.plot(loss, label=models[i])\n",
        "plt.xlabel('Epoch')  \n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.show()\n",
        "\n",
        "print(\"| Model | Loss |\")\n",
        "print(\"| - | - |\")\n",
        "for i in range(len(models)):\n",
        "  print(f\"| {models[i]} | {train_losses[i][-1]:.3f} |\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
