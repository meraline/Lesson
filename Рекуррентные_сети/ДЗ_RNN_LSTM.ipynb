{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель    Длина 25       Длина 75       Длина 150      \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (25) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Анатолий\\Documents\\GitHub\\Lesson\\Рекуррентные_сети\\ДЗ_RNN_LSTM.ipynb Ячейка 1\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m train_model(model, train_loader, criterion, optimizer)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# Оценка производительности на тестовых данных\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\Анатолий\\Documents\\GitHub\\Lesson\\Рекуррентные_сети\\ДЗ_RNN_LSTM.ipynb Ячейка 1\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(x\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, y\u001b[39m.\u001b[39;49mlong())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W0sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\functional.py:3328\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3325\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3326\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3328\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3329\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (25) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Шаг 1: Подготовка данных\n",
    "\n",
    "# Функция для генерации последовательностей x и соответствующих y\n",
    "def generate_sequences(length):\n",
    "    x = np.random.randint(0, 10, size=length)  # Генерируем случайную последовательность x\n",
    "    y = (x + x[0]) % 10  # Рассчитываем соответствующую последовательность y\n",
    "    return x, y\n",
    "\n",
    "# Генерация трех датасетов разной длины\n",
    "dataset_lengths = [25, 75, 150]\n",
    "datasets = []\n",
    "\n",
    "for length in dataset_lengths:\n",
    "    x_list, y_list = zip(*[generate_sequences(length) for _ in range(1000)])  # Создаем 1000 примеров для каждой длины\n",
    "    x_array = np.array(x_list, dtype=np.float32)\n",
    "    x_tensor = torch.tensor(x_array, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_list, dtype=torch.float32)\n",
    "    \n",
    "    # Проверяем, что x и y имеют одинаковое количество элементов\n",
    "    assert x_tensor.size(0) == y_tensor.size(0), \"Size mismatch between x and y\"\n",
    "    \n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Шаг 2: Определение архитектур моделей\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=64, batch_first=True)\n",
    "        # Убираем слой fc\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        # Возвращаем выход RNN, который имеет размерность [batch_size, seq_len, hidden_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class MyGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=1, hidden_size=64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Шаг 3: Обучение моделей\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "             # Изменение размерности x перед передачей в модель\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "            outputs = model(x.unsqueeze(2))\n",
    "            loss = criterion(outputs, y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Шаг 4: Оценка производительности моделей\n",
    "\n",
    "# Создаем заголовок для таблички с результатами\n",
    "print(f\"{'Модель':<10}{'Длина 25':<15}{'Длина 75':<15}{'Длина 150':<15}\")\n",
    "\n",
    "# Цикл по архитектурам моделей\n",
    "for model_type, ModelClass in [(\"RNN\", MyRNN), (\"LSTM\", MyLSTM), (\"GRU\", MyGRU)]:\n",
    "    row = f\"{model_type:<10}\"\n",
    "    \n",
    "    # Цикл по датасетам\n",
    "    for dataset in datasets:\n",
    "        train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        model = ModelClass()\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        train_model(model, train_loader, criterion, optimizer)\n",
    "        \n",
    "        # Оценка производительности на тестовых данных\n",
    "        test_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                outputs = model(x.unsqueeze(2))\n",
    "                loss = criterion(outputs, y.long().long())\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        row += f\"{test_loss/len(test_loader):<15.4f}{accuracy:<15.2f}\"\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 189802, 128), got [1, 1, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Анатолий\\Documents\\GitHub\\Lesson\\Рекуррентные_сети\\ДЗ_RNN_LSTM.ipynb Ячейка 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden(batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Инициализация скрытого состояния\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m output, hidden \u001b[39m=\u001b[39m model(X\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m), hidden)  \u001b[39m# Перестановка размерностей\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(), Y)  \u001b[39m# Перестановка размерностей и использование .squeeze()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Анатолий\\Documents\\GitHub\\Lesson\\Рекуррентные_сети\\ДЗ_RNN_LSTM.ipynb Ячейка 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, hidden):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%90%D0%BD%D0%B0%D1%82%D0%BE%D0%BB%D0%B8%D0%B9/Documents/GitHub/Lesson/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8/%D0%94%D0%97_RNN_LSTM.ipynb#W1sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:875\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    872\u001b[0m             hx \u001b[39m=\u001b[39m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[0;32m    873\u001b[0m         \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    874\u001b[0m         \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m--> 875\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:791\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    786\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    787\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    788\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    789\u001b[0m                        ):\n\u001b[0;32m    790\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 791\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_hidden_size(hidden[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_expected_hidden_size(\u001b[39minput\u001b[39;49m, batch_sizes),\n\u001b[0;32m    792\u001b[0m                            \u001b[39m'\u001b[39;49m\u001b[39mExpected hidden[0] size \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m, got \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    794\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Анатолий\\source\\repos\\PyTorchtest\\PyTorchtest\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:256\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_hidden_size\u001b[39m(\u001b[39mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[0;32m    254\u001b[0m                       msg: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mExpected hidden size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 256\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(expected_hidden_size, \u001b[39mlist\u001b[39m(hx\u001b[39m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 189802, 128), got [1, 1, 128]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Загрузка и предобработка текстовых данных\n",
    "text = open('nietzsche.txt', 'r', encoding='utf-8').read().lower()\n",
    "text = re.sub(r'[^a-z ]', '', text)  # Удаление всех символов, кроме строчных букв и пробелов\n",
    "\n",
    "# Создание списка уникальных символов и словарей для преобразования символов в индексы и наоборот\n",
    "chars = sorted(list(set(text))) \n",
    "char_indices = {ch: i for i, ch in enumerate(chars)}\n",
    "indices_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Параметры для создания последовательных образцов и их целевых символов\n",
    "maxlen = 20\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])  # Создание последовательных образцов длиной maxlen\n",
    "    next_chars.append(text[i + maxlen])    # Запись целевого символа для каждого образца\n",
    "\n",
    "# Создание тензоров для входных и целевых данных\n",
    "X = torch.zeros((len(sentences), maxlen, len(chars)), dtype=torch.float).cuda()  # Перемещение на GPU\n",
    "Y = torch.zeros((len(sentences)), dtype=torch.long).cuda()  # Перемещение на GPU\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1  # Применяем one-hot кодирование для символов\n",
    "    Y[i] = char_indices[next_chars[i]]\n",
    "\n",
    "# Определение модели LSTM и перемещение на GPU\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size).cuda(),\n",
    "                torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "# Инициализация модели и параметров тренировки\n",
    "model = LSTM(len(chars), 128, len(chars)).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Тренировка модели\n",
    "for epoch in range(100):\n",
    "    hidden = model.init_hidden(batch_size=1)  # Инициализация скрытого состояния\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output, hidden = model(X.permute(1, 0, 2), hidden)  # Перестановка размерностей\n",
    "    loss = criterion(output.permute(1, 0, 2).squeeze(), Y)  # Перестановка размерностей и использование .squeeze()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Эпоха: {}/{}. Потеря: {}'.format(epoch+1, 100, loss.item()))\n",
    "\n",
    "# Генерация текста с использованием обученной модели\n",
    "model.eval() \n",
    "generated = ''\n",
    "sentence = text[random.randint(0, len(text)-maxlen-1):][:maxlen]\n",
    "generated += sentence\n",
    "\n",
    "for i in range(maxlen):\n",
    "    x_inp = torch.tensor([char_indices[c] for c in sentence]).unsqueeze(0).cuda()\n",
    "    hid = model.init_hidden(batch_size=1)\n",
    "    output, hid = model(x_inp, hid)\n",
    "\n",
    "    pred_index = output.argmax().item()\n",
    "    predicted_char = indices_char[pred_index]\n",
    "\n",
    "    generated += predicted_char\n",
    "    sentence = sentence[1:] + predicted_char\n",
    "    \n",
    "print(generated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
