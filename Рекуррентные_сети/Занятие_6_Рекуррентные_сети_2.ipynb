{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iHah9Vq74t0e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import re\n",
        "import random\n",
        "import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (pyproject.toml): started\n",
            "  Building wheel for wget (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=4d733d0a9abc1b712f66c3a09f6ba3c143d67a36738f58fa3629c49b502f3215\n",
            "  Stored in directory: c:\\users\\анатолий\\appdata\\local\\pip\\cache\\wheels\\40\\b3\\0f\\a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-bs1_g342Nh",
        "outputId": "8e6e23d9-bf01-4530-dce5-1442ec442ad6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2023-09-21 22:14:49--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.16.46, 52.217.37.86, 52.217.74.86, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.16.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: 'nietzsche.txt'\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  8%  330K 2s\n",
            "    50K .......... .......... .......... .......... .......... 17%  337K 1s\n",
            "   100K .......... .......... .......... .......... .......... 25%  559K 1s\n",
            "   150K .......... .......... .......... .......... .......... 34%  830K 1s\n",
            "   200K .......... .......... .......... .......... .......... 42% 10,3M 1s\n",
            "   250K .......... .......... .......... .......... .......... 51%  351K 1s\n",
            "   300K .......... .......... .......... .......... .......... 59% 14,8M 0s\n",
            "   350K .......... .......... .......... .......... .......... 68% 10,3M 0s\n",
            "   400K .......... .......... .......... .......... .......... 76%  351K 0s\n",
            "   450K .......... .......... .......... .......... .......... 85% 10,5M 0s\n",
            "   500K .......... .......... .......... .......... .......... 93% 12,1M 0s\n",
            "   550K .......... .......... .......... ......               100% 11,1M=0,8s\n",
            "\n",
            "2023-09-21 22:14:51 (772 KB/s) - 'nietzsche.txt' saved [600901/600901]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3.amazonaws.com/text-datasets/nietzsche.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArE9Sysh5EDE",
        "outputId": "ae2671ba-c31b-4b86-e797-e012e256e790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length: 600893\n"
          ]
        }
      ],
      "source": [
        "with open('nietzsche.txt', encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('length:', len(text))\n",
        "text = re.sub('[^a-z ]', ' ', text)\n",
        "text = re.sub('\\s+', ' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ijyo7gcL49kz",
        "outputId": "738b82e2-1ea3-41d0-ba9f-422ae4b659af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'preface supposing that truth is a woman what then is there not ground for suspecting that all philos'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iNiH2xET5HxF"
      },
      "outputs": [],
      "source": [
        "INDEX_TO_CHAR = sorted(list(set(text)))\n",
        "CHAR_TO_INDEX = {c: i for i, c in enumerate(INDEX_TO_CHAR)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAer4W2RYfhr",
        "outputId": "964f954a-7b56-4ed4-a178-5d3fd87623cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " 'a': 1,\n",
              " 'b': 2,\n",
              " 'c': 3,\n",
              " 'd': 4,\n",
              " 'e': 5,\n",
              " 'f': 6,\n",
              " 'g': 7,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 'j': 10,\n",
              " 'k': 11,\n",
              " 'l': 12,\n",
              " 'm': 13,\n",
              " 'n': 14,\n",
              " 'o': 15,\n",
              " 'p': 16,\n",
              " 'q': 17,\n",
              " 'r': 18,\n",
              " 's': 19,\n",
              " 't': 20,\n",
              " 'u': 21,\n",
              " 'v': 22,\n",
              " 'w': 23,\n",
              " 'x': 24,\n",
              " 'y': 25,\n",
              " 'z': 26}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CHAR_TO_INDEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4EeJBub5ueL",
        "outputId": "6e6de311-5399-43e8-e49b-4e708eebeb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num sents: 193075\n"
          ]
        }
      ],
      "source": [
        "MAX_LEN = 40\n",
        "STEP = 3\n",
        "SENTENCES = []\n",
        "NEXT_CHARS = []\n",
        "for i in range(0, len(text) - MAX_LEN, STEP):\n",
        "    SENTENCES.append(text[i: i + MAX_LEN])\n",
        "    NEXT_CHARS.append(text[i + MAX_LEN])\n",
        "print('Num sents:', len(SENTENCES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHPHQII_6MUV",
        "outputId": "d48cb0da-193f-478e-e011-0cc3fa05c82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorization...\n"
          ]
        }
      ],
      "source": [
        "print('Vectorization...')\n",
        "X = torch.zeros((len(SENTENCES), MAX_LEN), dtype=int)\n",
        "Y = torch.zeros((len(SENTENCES)), dtype=int)\n",
        "for i, sentence in enumerate(SENTENCES):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t] = CHAR_TO_INDEX[char]\n",
        "    Y[i] = CHAR_TO_INDEX[NEXT_CHARS[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7MP7Jzi7PAN",
        "outputId": "c6d57fcb-d912-4b43-a2e8-17a46e4a8003"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[16, 18,  5,  6,  1,  3,  5,  0, 19, 21, 16, 16, 15, 19,  9, 14,  7,  0,\n",
              "          20,  8,  1, 20,  0, 20, 18, 21, 20,  8,  0,  9, 19,  0,  1,  0, 23, 15,\n",
              "          13,  1, 14,  0]]),\n",
              " tensor(23))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0:1], Y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4XKb2CyB6nwL"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=512\n",
        "dataset = torch.utils.data.TensorDataset(X, Y)\n",
        "data = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "psIcSGM27YPL"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.embedding = nn.Embedding(dictionary_size, embedding_size)\n",
        "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
        "        self.output = nn.Linear(num_hiddens, num_classes)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        out = self.embedding(X)\n",
        "        _, state = self.hidden(out)\n",
        "        predictions = self.output(state[0].squeeze())\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wHDuSE8A7ssc"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork(nn.GRU, len(CHAR_TO_INDEX), 64, 128, len(CHAR_TO_INDEX))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvKPD9L9zJal",
        "outputId": "8a69e7a2-ef45-4acb-af30-58ed5b20f316"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([193075, 40])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTCG-ESC74UK",
        "outputId": "183a612f-2a3a-4a5a-87a7-f355a47f924f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.0791, -0.2085, -0.0455, -0.1693, -0.2564,  0.0378, -0.1432, -0.1375,\n",
              "        -0.0876, -0.0774, -0.0693, -0.1199,  0.1950,  0.0108, -0.1786,  0.0804,\n",
              "         0.0004,  0.1309, -0.0236,  0.0134,  0.0097,  0.1423, -0.1204, -0.0674,\n",
              "        -0.0557,  0.1333,  0.0175], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(X[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gch6FQl8x6Hj"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(len(INDEX_TO_CHAR), 15)\n",
        "rnn = nn.LSTM(15,128, batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJsUqfxYeq0b",
        "outputId": "fef78aac-7f25-4225-f2e0-37a661d91676"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([10, 40, 128]),\n",
              " 2,\n",
              " torch.Size([1, 10, 128]),\n",
              " torch.Size([1, 10, 128]))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "o, s = rnn(embedding(X[0:10]))\n",
        "o.shape, len(s), s[0].shape, s[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qbiqECBCP4Bv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([10, 40, 128]), 1, torch.Size([10, 128]))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnn = nn.GRU(15,128, batch_first=True)\n",
        "o, s = rnn(embedding(X[0:10]))\n",
        "o.shape, len(s), s[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "evDHlyNOykBr"
      },
      "outputs": [],
      "source": [
        "o, s = rnn(embedding(X[0:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hYTukwkykHJ",
        "outputId": "f75b46be-f6ab-4528-8e1c-b0e47dba0b06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([10, 40, 128]), torch.Size([10, 128]), None)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# o.shape, s[0].shape, s[1].shape\n",
        "o.shape, s[0].shape, s[0].shape if len(s) > 1 else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jbeKFkwdFclg"
      },
      "outputs": [],
      "source": [
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZQpkKJV_76dq"
      },
      "outputs": [],
      "source": [
        "def sample(preds):\n",
        "     # Преобразовать preds в одномерный тензор\n",
        "    preds = preds.view(-1)\n",
        "    softmaxed = torch.softmax(preds, 0)\n",
        "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
        "    return probas.argmax()\n",
        "\n",
        "def generate_text():\n",
        "    start_index = random.randint(0, len(text) - MAX_LEN - 1)\n",
        "\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + MAX_LEN]\n",
        "    generated += sentence\n",
        "\n",
        "    for i in range(MAX_LEN):\n",
        "        x_pred = torch.zeros((1, MAX_LEN), dtype=int)\n",
        "        for t, char in enumerate(generated[-MAX_LEN:]):\n",
        "            x_pred[0, t] = CHAR_TO_INDEX[char]\n",
        "\n",
        "        preds = model(x_pred.cuda())[0].cpu()\n",
        "        next_char = INDEX_TO_CHAR[sample(preds)]\n",
        "        generated = generated + next_char\n",
        "\n",
        "    print(generated[:MAX_LEN] + '|' + generated[MAX_LEN:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EV09Ast97aQ",
        "outputId": "385ca42f-74ee-41a8-8aeb-3d12d30d195b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tenance of a definite mode of life for e|                                        \n"
          ]
        }
      ],
      "source": [
        "generate_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hylQYY8H_Lw2"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qshorynU9-Cx",
        "outputId": "e2b45154-9cf2-4799-d66a-2eb9fc5a92fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Time: 6.138, Train loss: 1.724\n",
            "t for example sensual pleasure preferred|                                        \n",
            "Epoch 1. Time: 6.352, Train loss: 1.597\n",
            "laim is that through the medium of profo|                                        \n",
            "Epoch 2. Time: 6.367, Train loss: 1.518\n",
            "ilry we have in us our disgust at the cl|                                        \n",
            "Epoch 3. Time: 6.375, Train loss: 1.463\n",
            "but you misunderstand him when you compl|                                        \n",
            "Epoch 4. Time: 6.346, Train loss: 1.420\n",
            "el the breaking of the too rigid symmetr|                                        \n",
            "Epoch 5. Time: 6.427, Train loss: 1.388\n",
            "n distress the germans invented gunpowde|                                        \n",
            "Epoch 6. Time: 6.481, Train loss: 1.361\n",
            "nctity to question which or to account f|                                        \n",
            "Epoch 7. Time: 6.392, Train loss: 1.339\n",
            " stands all the colder and more reserved|                                        \n",
            "Epoch 8. Time: 6.384, Train loss: 1.319\n",
            "ge as weakness of will consequently in t|                                        \n",
            "Epoch 9. Time: 6.387, Train loss: 1.302\n",
            "rac of ideas of the most varied origin s|                                        \n",
            "Epoch 10. Time: 6.460, Train loss: 1.287\n",
            " regarding salvation all notions that ar|                                        \n",
            "Epoch 11. Time: 6.450, Train loss: 1.274\n",
            "still achieve much the german lets himse|                                        \n",
            "Epoch 12. Time: 6.372, Train loss: 1.263\n",
            "e french revolution wherever the religio|                                        \n",
            "Epoch 13. Time: 6.391, Train loss: 1.251\n",
            "europe they long to be finally settled a|                                        \n",
            "Epoch 14. Time: 6.383, Train loss: 1.241\n",
            "e day perhaps such will actually be our |                                        \n",
            "Epoch 15. Time: 6.405, Train loss: 1.232\n",
            "n critic and dogmatist and historian and|                                        \n",
            "Epoch 16. Time: 6.609, Train loss: 1.223\n",
            "ophical persons a mystery and during cou|                                        \n",
            "Epoch 17. Time: 6.712, Train loss: 1.215\n",
            " self glorification and self conceitedne|                                        \n",
            "Epoch 18. Time: 6.569, Train loss: 1.208\n",
            "e of his suffering he knows more than th|                                        \n",
            "Epoch 19. Time: 7.422, Train loss: 1.200\n",
            " individual himself is held fast in the |                                        \n",
            "Epoch 20. Time: 6.817, Train loss: 1.194\n",
            "ement in it by the result no the one who|                                        \n",
            "Epoch 21. Time: 7.515, Train loss: 1.188\n",
            "ng to the average man if notwithstanding|                                        \n",
            "Epoch 22. Time: 6.567, Train loss: 1.182\n",
            "f ecstacies they set themselves before t|                                        \n",
            "Epoch 23. Time: 6.520, Train loss: 1.178\n",
            "rity the extent and towering structure o|                                        \n",
            "Epoch 24. Time: 6.404, Train loss: 1.171\n",
            "riousness and if nevertheless our honest|                                        \n",
            "Epoch 25. Time: 6.542, Train loss: 1.165\n",
            " the political formula the democratic mo|                                        \n",
            "Epoch 26. Time: 6.318, Train loss: 1.160\n",
            " reserved expressly for us and allow our|                                        \n",
            "Epoch 27. Time: 6.438, Train loss: 1.157\n",
            "dially interested by nothing whatever in|                                        \n",
            "Epoch 28. Time: 6.779, Train loss: 1.151\n",
            " one of the most usual errors of deducti|                                        \n",
            "Epoch 29. Time: 6.825, Train loss: 1.147\n",
            "nefit of woman when napoleon gave the to|                                        \n",
            "Epoch 30. Time: 7.096, Train loss: 1.144\n",
            "ention in the background is only an addi|                                        \n",
            "Epoch 31. Time: 6.637, Train loss: 1.139\n",
            "n can suffer the chilling certainty with|                                        \n",
            "Epoch 32. Time: 7.693, Train loss: 1.135\n",
            "al element that can be grasped the tree |                                        \n",
            "Epoch 33. Time: 6.471, Train loss: 1.131\n",
            "oes he experience remorse and the stings|                                        \n",
            "Epoch 34. Time: 6.772, Train loss: 1.128\n",
            "let us not be ungrateful to it although |                                        \n",
            "Epoch 35. Time: 6.610, Train loss: 1.124\n",
            "ted to give men uneasiness yet everythin|                                        \n",
            "Epoch 36. Time: 6.355, Train loss: 1.122\n",
            "physicists perhaps not only indian wars |                                        \n",
            "Epoch 37. Time: 6.468, Train loss: 1.119\n",
            "continue to live and the other dies inst|                                        \n",
            "Epoch 38. Time: 6.426, Train loss: 1.114\n",
            "rden and disgust upon himself that he pe|                                        \n",
            "Epoch 39. Time: 6.407, Train loss: 1.112\n",
            "int for point with the philosophy of epi|                                        \n",
            "Epoch 40. Time: 6.469, Train loss: 1.109\n",
            "s to be fashioned bruised forged stretch|                                        \n",
            "Epoch 41. Time: 6.276, Train loss: 1.106\n",
            " of delight as commander l effet c est m|                                        \n",
            "Epoch 42. Time: 6.471, Train loss: 1.103\n",
            "y so apart so alone renouncing all i lov|                                        \n",
            "Epoch 43. Time: 6.371, Train loss: 1.099\n",
            "his crushing of self this mockery of one|                                        \n",
            "Epoch 44. Time: 6.399, Train loss: 1.097\n",
            "if you please teems with the naivetes of|                                        \n",
            "Epoch 45. Time: 6.487, Train loss: 1.095\n",
            "row from the beginning a sort of girl an|                                        \n",
            "Epoch 46. Time: 6.365, Train loss: 1.092\n",
            "olent beings might look pale and dwarfed|                                        \n",
            "Epoch 47. Time: 6.454, Train loss: 1.089\n",
            "rth as one might say at the right moment|                                        \n",
            "Epoch 48. Time: 6.371, Train loss: 1.087\n",
            "f new germanism could enter into relatio|                                        \n",
            "Epoch 49. Time: 6.388, Train loss: 1.085\n",
            "ible stable and the like to such an exte|                                        \n",
            "Epoch 50. Time: 6.484, Train loss: 1.082\n",
            "s spiritualization the english coarsenes|                                        \n",
            "Epoch 51. Time: 6.306, Train loss: 1.080\n",
            "lizing of man into a pigmy with equal ri|                                        \n",
            "Epoch 52. Time: 6.385, Train loss: 1.079\n",
            "ophy has gradually sunk the remnant of p|                                        \n",
            "Epoch 53. Time: 6.400, Train loss: 1.077\n",
            "knowledge of a man for example stood he |                                        \n",
            "Epoch 54. Time: 6.387, Train loss: 1.075\n",
            "otive into account rates both cases alik|                                        \n",
            "Epoch 55. Time: 6.481, Train loss: 1.071\n",
            "ere the error is always greatest there n|                                        \n",
            "Epoch 56. Time: 6.297, Train loss: 1.071\n",
            " to express it more agreeably thus man w|                                        \n",
            "Epoch 57. Time: 6.381, Train loss: 1.069\n",
            " will paralysis of will where do we not |                                        \n",
            "Epoch 58. Time: 6.381, Train loss: 1.068\n",
            "he bedlamite hope that because you are a|                                        \n",
            "Epoch 59. Time: 6.372, Train loss: 1.066\n",
            "thoven stendhal heinrich heine schopenha|                                        \n",
            "Epoch 60. Time: 6.372, Train loss: 1.064\n",
            "ness with their lot and condition peace |                                        \n",
            "Epoch 61. Time: 6.488, Train loss: 1.063\n",
            " the slave is still left in woman for in|                                        \n",
            "Epoch 62. Time: 6.387, Train loss: 1.062\n",
            "irit which is in an equal degree self ma|                                        \n",
            "Epoch 63. Time: 6.373, Train loss: 1.060\n",
            " not that sincere austere slave faith by|                                        \n",
            "Epoch 64. Time: 6.394, Train loss: 1.058\n",
            " their century and it is the century of |                                        \n",
            "Epoch 65. Time: 6.469, Train loss: 1.057\n",
            "f the unalterable facts regarding man hi|                                        \n",
            "Epoch 66. Time: 6.776, Train loss: 1.056\n",
            "to our neighbourhood and friendship to l|                                        \n",
            "Epoch 67. Time: 6.783, Train loss: 1.053\n",
            "ve is he finds that it rather destroys t|                                        \n",
            "Epoch 68. Time: 6.951, Train loss: 1.054\n",
            "nion but also to a bad and unjust one th|                                        \n",
            "Epoch 69. Time: 7.085, Train loss: 1.051\n",
            "tity in the light of which his own image|                                        \n",
            "Epoch 70. Time: 6.711, Train loss: 1.050\n",
            "pth as of the mould something uncommunic|                                        \n",
            "Epoch 71. Time: 6.685, Train loss: 1.049\n",
            "rless should be religious also the every|                                        \n",
            "Epoch 72. Time: 6.625, Train loss: 1.049\n",
            "the ascendancy nay literally the suprema|                                        \n",
            "Epoch 73. Time: 7.374, Train loss: 1.046\n",
            " precisely that society is not allowed t|                                        \n",
            "Epoch 74. Time: 6.503, Train loss: 1.046\n",
            " in so far as it be delusive but in trut|                                        \n",
            "Epoch 75. Time: 6.536, Train loss: 1.044\n",
            "est detail of morality inasmuch as insig|                                        \n",
            "Epoch 76. Time: 7.829, Train loss: 1.044\n",
            "eaves all seared all dry oh friends no m|                                        \n",
            "Epoch 77. Time: 8.358, Train loss: 1.043\n",
            "e think that all our feelings and doings|                                        \n",
            "Epoch 78. Time: 7.688, Train loss: 1.043\n",
            "thousand years old we should have in him|                                        \n",
            "Epoch 79. Time: 6.497, Train loss: 1.044\n",
            "s his imagination is exalted he is alway|                                        \n",
            "Epoch 80. Time: 7.055, Train loss: 1.039\n",
            "self the assumption of the freedom of th|                                        \n",
            "Epoch 81. Time: 7.724, Train loss: 1.039\n",
            " then have i evolved for myself the free|                                        \n",
            "Epoch 82. Time: 6.174, Train loss: 1.038\n",
            "om concerning things in mere words and i|                                        \n",
            "Epoch 83. Time: 6.560, Train loss: 1.038\n",
            "th one another in the new generation whi|                                        \n",
            "Epoch 84. Time: 6.638, Train loss: 1.036\n",
            "nemy is necessary and he is found in the|                                        \n",
            "Epoch 85. Time: 6.964, Train loss: 1.036\n",
            "ble impression there are certain excepti|                                        \n",
            "Epoch 86. Time: 6.836, Train loss: 1.034\n",
            "t could be lived so that at first the ol|                                        \n",
            "Epoch 87. Time: 6.505, Train loss: 1.035\n",
            " of praise is in many cases merely polit|                                        \n",
            "Epoch 88. Time: 6.620, Train loss: 1.035\n",
            "ke of them indeed one might ask as a phy|                                        \n",
            "Epoch 89. Time: 6.465, Train loss: 1.032\n",
            "mes that i compare my state at the prese|                                        \n",
            "Epoch 90. Time: 6.774, Train loss: 1.039\n",
            "risto european morality suffers from an |                                        \n",
            "Epoch 91. Time: 6.505, Train loss: 1.033\n",
            "s a middle ground to this which a man of|                                        \n",
            "Epoch 92. Time: 6.334, Train loss: 1.031\n",
            "ating disposing and constructing in the |                                        \n",
            "Epoch 93. Time: 6.588, Train loss: 1.031\n",
            " the spoken word arcubalista into armbru|                                        \n",
            "Epoch 94. Time: 6.551, Train loss: 1.030\n",
            "s most to retain us in this simplified t|                                        \n",
            "Epoch 95. Time: 6.667, Train loss: 1.030\n",
            " eye has exercised its acuteness and pro|                                        \n",
            "Epoch 96. Time: 6.767, Train loss: 1.031\n",
            "mes possible to entertain the idea of an|                                        \n",
            "Epoch 97. Time: 6.536, Train loss: 1.028\n",
            "ous sign of the lack thereof it is not t|                                        \n",
            "Epoch 98. Time: 6.429, Train loss: 1.029\n",
            " for it it was precisely owing to moral |                                        \n",
            "Epoch 99. Time: 6.366, Train loss: 1.027\n",
            "y philosophy has been a long tragedy in |                                        \n"
          ]
        }
      ],
      "source": [
        "for ep in range(100):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "\n",
        "    model.train()\n",
        "    for X_b, y_b in data:\n",
        "        X_b, y_b = X_b.cuda(), y_b.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        answers = model(X_b)\n",
        "        loss = criterion(answers, y_b)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1\n",
        "\n",
        "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
        "    model.eval()\n",
        "    generate_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzUTHB9O_H6J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Занятие 6. Рекуррентные сети 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
